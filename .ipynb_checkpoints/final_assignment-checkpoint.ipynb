{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca693bd1-1529-4fea-bf2f-4a54ea05cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36dfbb0c-c490-4f9d-98dd-6887e6c329cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0bc8eff-6959-4ddb-9818-bd8e05fd17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable (first column)\n",
    "y_train = df.iloc[:, 0]  # This selects the first column\n",
    "# Create the feature set by dropping the first column\n",
    "x_train = df.drop(df.columns[0], axis=1)\n",
    "y_one_hot = np.zeros((y_train.size,10 ))\n",
    "y_one_hot[np.arange(y_train.size), y_train] = 1\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "x_train = sigmoid(x_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a741d1c4-63c4-460f-93ec-6800874c36d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    4\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5   \n",
      "1     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5   \n",
      "2     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5   \n",
      "3     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5   \n",
      "4     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5   \n",
      "\n",
      "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0     0.5  ...       0.5       0.5       0.5       0.5       0.5       0.5   \n",
      "1     0.5  ...       0.5       0.5       0.5       0.5       0.5       0.5   \n",
      "2     0.5  ...       0.5       0.5       0.5       0.5       0.5       0.5   \n",
      "3     0.5  ...       0.5       0.5       0.5       0.5       0.5       0.5   \n",
      "4     0.5  ...       0.5       0.5       0.5       0.5       0.5       0.5   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  \n",
      "0       0.5       0.5       0.5       0.5  \n",
      "1       0.5       0.5       0.5       0.5  \n",
      "2       0.5       0.5       0.5       0.5  \n",
      "3       0.5       0.5       0.5       0.5  \n",
      "4       0.5       0.5       0.5       0.5  \n",
      "\n",
      "[5 rows x 784 columns]\n",
      "(42000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.head())\n",
    "print(x_train.head())\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26491b6f-a4ec-4142-aa46-5ce3f68df3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = x_train.shape[1]  # Number of features in X_train\n",
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ba7801-406b-464e-9ab0-a8a846423eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer = np.zeros(10)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f93302-b28b-4401-82ea-abc48b809bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size , output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.input = None  # Store the input data here\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.dL_dW = None\n",
    "        self.dL_db = None\n",
    "    \n",
    "    # Example forward method\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        # Compute the forward pass (just an example, actual implementation may vary)\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "    \n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        # dL_dy: Gradient of the loss with respect to the output of this layer\n",
    "        # Returns dL_dx: Gradient of the loss with respect to the input of this layer\n",
    "        \n",
    "        # Correct calculation of gradients\n",
    "        dL_dx = np.dot(dL_dy, self.weights.T)  # Corrected to match dimensions\n",
    "        self.dL_dW = np.dot(self.input.T, dL_dy)\n",
    "        self.dL_db = np.sum(dL_dy, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights and biases here or return gradients for external update\n",
    "        # For example:\n",
    "        # self.W -= learning_rate * dL_dW\n",
    "        # self.b -= learning_rate * dL_db\n",
    "        \n",
    "        return dL_dx\n",
    "\n",
    "# Assuming ReLU, Softmax, and other components are correctly implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38b3a81-cd18-4d7d-ad93-2b9c06b21e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        out = np.maximum(0, X)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        dX = np.zeros_like(dL_dy)  \n",
    "        \n",
    "        # Only propagate gradients for inputs that were greater than zero\n",
    "        dX[~self.mask] = dL_dy[~self.mask]  \n",
    "        # Set gradients where input was positive\n",
    "        \n",
    "        return dX\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        self.out = 1 / (1 + np.exp(-X))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        return dL_dy * self.out * (1 - self.out)\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, X):\n",
    "        self.out = np.tanh(X)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        return dL_dy * (1 - self.out**2)\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.out = None  # This will hold the output of the forward pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Applies the softmax function to the input.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Softmax probabilities of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))  # For numerical stability\n",
    "        self.out = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the loss with respect to the input of the softmax layer.\n",
    "\n",
    "        Args:\n",
    "            dL_dy (numpy.ndarray): Gradient of the loss with respect to the output of the softmax layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input of the softmax layer.\n",
    "        \"\"\"\n",
    "        # dL_dy is of shape (batch_size, num_classes)\n",
    "        batch_size = dL_dy.shape[0]\n",
    "        num_classes = dL_dy.shape[1]\n",
    "\n",
    "        # Create a gradient output with the same shape as dL_dy\n",
    "        dL_dx = np.zeros((batch_size, num_classes))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # For each sample, compute the Jacobian of the softmax function\n",
    "            s = self.out[i]  # Softmax output for the current sample\n",
    "            jacobian = np.diag(s) - np.outer(s, s)  # Jacobian matrix of the softmax function\n",
    "            dL_dx[i] = np.dot(jacobian, dL_dy[i])  # Apply the chain rule\n",
    "\n",
    "        return dL_dx  # Shape will be (batch_size, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1df09784-e006-4471-a9d0-e5b59c3f0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y_true, y_pred):\n",
    "\n",
    "        # Calculate the mean squared error\n",
    "        self.loss = np.mean((y_true - y_pred) ** 2)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, y_true, y_pred):\n",
    "\n",
    "        # Number of samples\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        # Calculate the gradient (derivative of the loss)\n",
    "        dL_dy_pred = (2 * (y_pred - y_true)) / m\n",
    "        return dL_dy_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3c3632-d2ab-4189-b25a-f2b3f65d6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_one_hot, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the Cross-Entropy loss.\n",
    "\n",
    "        Args:\n",
    "            y_one_hot (numpy.ndarray): True labels (one-hot encoded).\n",
    "            y_pred (numpy.ndarray): Predicted probabilities from the model.\n",
    "\n",
    "        Returns:\n",
    "            float: The computed cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # Ensure numerical stability with a small constant\n",
    "        epsilon = 1e-15\n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        loss = -np.mean(np.sum(y_one_hot * np.log(y_pred), axis=1))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y_one_hot, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the Cross-Entropy loss with respect to the predicted probabilities.\n",
    "\n",
    "        Args:\n",
    "            y_one_hot (numpy.ndarray): True labels (one-hot encoded).\n",
    "            y_pred (numpy.ndarray): Predicted probabilities from the model.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the predicted probabilities.\n",
    "        \"\"\"\n",
    "        # Number of samples\n",
    "        m = y_one_hot.shape[0]\n",
    "        \n",
    "        # Gradient of the loss with respect to predicted probabilities\n",
    "        dL_dy_pred = - (y_one_hot / y_pred) / m\n",
    "        return dL_dy_pred\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a12860f2-30f1-411d-a6dd-6c54ba76b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the SGD optimizer.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): The learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, model):\n",
    "        \"\"\"\n",
    "        Updates the model parameters using the gradients stored in the model.\n",
    "\n",
    "        Args:\n",
    "            model (Model): The model containing parameters and gradients.\n",
    "        \"\"\"\n",
    "        for layer in model.layers:\n",
    "            if hasattr(layer, 'weights'):\n",
    "                layer.weights -= self.learning_rate * layer.dL_dW\n",
    "                layer.biases -= self.learning_rate * layer.dL_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d7e9608-7dcd-4338-bc26-8b95270a8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss_fn = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Adds a layer to the model.\n",
    "\n",
    "        Args:\n",
    "            layer: An instance of a layer class (e.g., Linear, ReLU, Softmax).\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, loss_fn, optimizer):\n",
    "        \"\"\"\n",
    "        Compiles the model with a loss function and an optimizer.\n",
    "\n",
    "        Args:\n",
    "            loss_fn: An instance of a loss function class (e.g., CrossEntropyLoss).\n",
    "            optimizer: An instance of an optimizer class (e.g., SGD).\n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output from the final layer.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, y_one_hot, y_pred):\n",
    "        \"\"\"\n",
    "        Performs the backward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            y_one_hot (numpy.ndarray): True labels.\n",
    "            y_pred (numpy.ndarray): Predicted outputs.\n",
    "        \"\"\"\n",
    "        # Compute the initial gradient from the loss function\n",
    "        dL_dout = self.loss_fn.backward(y_one_hot, y_pred)\n",
    "        # Backpropagate through the layers\n",
    "        for layer in reversed(self.layers):\n",
    "            dL_dout = layer.backward(dL_dout)\n",
    "\n",
    "    def train(self, X_train, y_one_hot, epochs=10, batch_size=10):\n",
    "        \"\"\"\n",
    "        Trains the model on the training data.\n",
    "\n",
    "        Args:\n",
    "            X_train (numpy.ndarray): Training input data.\n",
    "            y_one_hot (numpy.ndarray): Training labels (one-hot encoded).\n",
    "            epochs (int): Number of training epochs.\n",
    "            batch_size (int): Size of each training batch (default is 10).\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                X_batch = X_train[i:i + batch_size]\n",
    "                y_batch = y_one_hot[i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.loss_fn.forward(y_batch, y_pred)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(y_batch, y_pred)\n",
    "\n",
    "                # Update parameters using the optimizer\n",
    "                self.optimizer.step(self)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Makes predictions for the input data.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted outputs.\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the test data.\n",
    "\n",
    "        Args:\n",
    "            X_test (numpy.ndarray): Test input data.\n",
    "            y_test (numpy.ndarray): Test labels (one-hot encoded).\n",
    "\n",
    "        Returns:\n",
    "            float: Loss on the test data.\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X_test)\n",
    "        loss = self.loss_fn.forward(y_test, y_pred)\n",
    "        return loss\n",
    "\n",
    "# have to write a load and save method\n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Saves the model to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The name of the file to save the model.\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if hasattr(layer, 'weights'):\n",
    "                np.savetxt(f\"{filename}_layer_{i}_weights.csv\", layer.weights, delimiter=\",\")\n",
    "            if hasattr(layer, 'biases'):\n",
    "                np.savetxt(f\"{filename}_layer_{i}_biases.csv\", layer.biases, delimiter=\",\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        Loads the weights and biases from CSV files and the training data.\n",
    "        Computes predictions and saves them to a CSV file.\n",
    "    \n",
    "        Args:\n",
    "            filename (str): The base name of the files for weights and biases.\n",
    "        \"\"\"\n",
    "        # Load weights and biases for each layer\n",
    "        weights_trained = []\n",
    "        biases_trained = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if hasattr(layer, 'weights'): \n",
    "                weights_layer = np.loadtxt(f\"{filename}_layer_{i}_weights.csv\", delimiter=\",\")\n",
    "                weights_trained.append(weights_layer)\n",
    "            if hasattr(layer, 'biases'):\n",
    "                biases_layer = np.loadtxt(f\"{filename}_layer_{i}_biases.csv\", delimiter=\",\")\n",
    "                biases_trained.append(biases_layer)\n",
    "            else :\n",
    "                weights_trained.append(None) \n",
    "                biases_trained.append(None)\n",
    "    \n",
    "        # Load X_train data (assuming it's stored in a specific CSV file)\n",
    "        X_train_for_pred = x_train\n",
    "    \n",
    "        # Forward pass through each layer\n",
    "        a_prev = X_train_for_pred\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Linear):  # If the layer is a Linear layer\n",
    "                z = np.dot(a_prev, weights_trained[i]) + biases_trained[i]  # Linear transformation\n",
    "                a_prev = z  # Pass the linear output to the activation function\n",
    "            elif isinstance(layer, ReLU):  # If the layer is a ReLU layer\n",
    "                a_prev = layer.forward(a_prev)  # Apply ReLU activation\n",
    "            elif isinstance(layer, Softmax):  # If the layer is a Softmax layer\n",
    "                a_prev = layer.forward(a_prev)  # Apply Softmax activation\n",
    "    \n",
    "        y_predicted = a_prev  # Final predictions\n",
    "        header_row_y_pred = np.arange(0,10)\n",
    "        y_predicted = np.insert(y_predicted, 0, header_row_y_pred, axis=0)\n",
    "\n",
    "        # Save predictions to CSV\n",
    "        np.savetxt(\"y_predicted.csv\", y_predicted, delimiter=\",\")\n",
    "        print(\"Predictions saved to y_predicted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa02aaf-5772-4907-b9c3-cb22ef40eb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.5836\n",
      "Epoch [2/100], Loss: 0.3024\n",
      "Epoch [3/100], Loss: 0.2254\n",
      "Epoch [4/100], Loss: 0.2038\n",
      "Epoch [5/100], Loss: 0.1936\n",
      "Epoch [6/100], Loss: 0.1821\n",
      "Epoch [7/100], Loss: 0.1732\n",
      "Epoch [8/100], Loss: 0.1603\n",
      "Epoch [9/100], Loss: 0.1530\n",
      "Epoch [10/100], Loss: 0.1386\n",
      "Epoch [11/100], Loss: 0.1286\n",
      "Epoch [12/100], Loss: 0.1229\n",
      "Epoch [13/100], Loss: 0.1162\n",
      "Epoch [14/100], Loss: 0.1073\n",
      "Epoch [15/100], Loss: 0.0994\n",
      "Epoch [16/100], Loss: 0.0957\n",
      "Epoch [17/100], Loss: 0.0856\n",
      "Epoch [18/100], Loss: 0.0797\n",
      "Epoch [19/100], Loss: 0.0743\n",
      "Epoch [20/100], Loss: 0.0726\n",
      "Epoch [21/100], Loss: 0.0686\n",
      "Epoch [22/100], Loss: 0.0664\n",
      "Epoch [23/100], Loss: 0.0621\n",
      "Epoch [24/100], Loss: 0.0586\n",
      "Epoch [25/100], Loss: 0.0532\n",
      "Epoch [26/100], Loss: 0.0517\n",
      "Epoch [27/100], Loss: 0.0487\n",
      "Epoch [28/100], Loss: 0.0459\n",
      "Epoch [29/100], Loss: 0.0425\n",
      "Epoch [30/100], Loss: 0.0395\n",
      "Epoch [31/100], Loss: 0.0383\n",
      "Epoch [32/100], Loss: 0.0379\n",
      "Epoch [33/100], Loss: 0.0363\n",
      "Epoch [34/100], Loss: 0.0335\n",
      "Epoch [35/100], Loss: 0.0345\n",
      "Epoch [36/100], Loss: 0.0338\n",
      "Epoch [37/100], Loss: 0.0327\n",
      "Epoch [38/100], Loss: 0.0312\n",
      "Epoch [39/100], Loss: 0.0288\n",
      "Epoch [40/100], Loss: 0.0281\n",
      "Epoch [41/100], Loss: 0.0266\n",
      "Epoch [42/100], Loss: 0.0277\n",
      "Epoch [43/100], Loss: 0.0266\n",
      "Epoch [44/100], Loss: 0.0250\n",
      "Epoch [45/100], Loss: 0.0248\n",
      "Epoch [46/100], Loss: 0.0245\n",
      "Epoch [47/100], Loss: 0.0243\n",
      "Epoch [48/100], Loss: 0.0236\n",
      "Epoch [49/100], Loss: 0.0227\n",
      "Epoch [50/100], Loss: 0.0198\n",
      "Epoch [51/100], Loss: 0.0186\n",
      "Epoch [52/100], Loss: 0.0188\n",
      "Epoch [53/100], Loss: 0.0182\n",
      "Epoch [54/100], Loss: 0.0170\n",
      "Epoch [55/100], Loss: 0.0168\n",
      "Epoch [56/100], Loss: 0.0166\n",
      "Epoch [57/100], Loss: 0.0163\n",
      "Epoch [58/100], Loss: 0.0163\n",
      "Epoch [59/100], Loss: 0.0158\n",
      "Epoch [60/100], Loss: 0.0155\n",
      "Epoch [61/100], Loss: 0.0153\n",
      "Epoch [62/100], Loss: 0.0145\n",
      "Epoch [63/100], Loss: 0.0146\n",
      "Epoch [64/100], Loss: 0.0141\n",
      "Epoch [65/100], Loss: 0.0139\n",
      "Epoch [66/100], Loss: 0.0139\n",
      "Epoch [67/100], Loss: 0.0134\n",
      "Epoch [68/100], Loss: 0.0129\n",
      "Epoch [69/100], Loss: 0.0128\n",
      "Epoch [70/100], Loss: 0.0120\n",
      "Epoch [71/100], Loss: 0.0124\n",
      "Epoch [72/100], Loss: 0.0123\n",
      "Epoch [73/100], Loss: 0.0118\n",
      "Epoch [74/100], Loss: 0.0118\n",
      "Epoch [75/100], Loss: 0.0113\n",
      "Epoch [76/100], Loss: 0.0111\n",
      "Epoch [77/100], Loss: 0.0115\n",
      "Epoch [78/100], Loss: 0.0111\n",
      "Epoch [79/100], Loss: 0.0107\n",
      "Epoch [80/100], Loss: 0.0106\n",
      "Epoch [81/100], Loss: 0.0106\n",
      "Epoch [82/100], Loss: 0.0104\n",
      "Epoch [83/100], Loss: 0.0100\n",
      "Epoch [84/100], Loss: 0.0106\n",
      "Epoch [85/100], Loss: 0.0096\n",
      "Epoch [86/100], Loss: 0.0095\n",
      "Epoch [87/100], Loss: 0.0099\n",
      "Epoch [88/100], Loss: 0.0095\n",
      "Epoch [89/100], Loss: 0.0102\n",
      "Epoch [90/100], Loss: 0.0093\n",
      "Epoch [91/100], Loss: 0.0084\n",
      "Epoch [92/100], Loss: 0.0094\n",
      "Epoch [93/100], Loss: 0.0098\n",
      "Epoch [94/100], Loss: 0.0092\n",
      "Epoch [95/100], Loss: 0.0093\n",
      "Epoch [96/100], Loss: 0.0090\n",
      "Epoch [97/100], Loss: 0.0089\n",
      "Epoch [98/100], Loss: 0.0086\n",
      "Epoch [99/100], Loss: 0.0085\n",
      "Epoch [100/100], Loss: 0.0083\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "model = Model()\n",
    "\n",
    "# Add layers to the model (assuming you have defined layers like Linear, ReLU, etc.)\n",
    "model.add_layer(Linear(input_size=784, output_size=128))  # Example layer\n",
    "model.add_layer(ReLU())\n",
    "model.add_layer(Linear(input_size=128,output_size=10))   # Output layer # have to give argument input size 128 and output size 10\n",
    "model.add_layer(Softmax())\n",
    "\n",
    "# Compile the model with a loss function and an optimizer\n",
    "model.compile(loss_fn=CrossEntropyLoss(), optimizer=SGD(learning_rate=0.01))\n",
    "\n",
    "# Train the model on your training data\n",
    "model.train(x_train, y_one_hot, epochs=100, batch_size=20)\n",
    "\n",
    "# Save the model to CSV files\n",
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "689631e8-dfdc-4258-834e-b64073ca6be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to y_predicted.csv\n"
     ]
    }
   ],
   "source": [
    "model.load('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c347950-7e32-4131-90d2-1c3f5edd8e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.860000e-07</td>\n",
       "      <td>9.990000e-01</td>\n",
       "      <td>8.420000e-06</td>\n",
       "      <td>1.340000e-05</td>\n",
       "      <td>2.100000e-05</td>\n",
       "      <td>3.390000e-05</td>\n",
       "      <td>7.740000e-05</td>\n",
       "      <td>7.080000e-06</td>\n",
       "      <td>1.140000e-03</td>\n",
       "      <td>2.060000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.900000e-16</td>\n",
       "      <td>2.830000e-07</td>\n",
       "      <td>2.560000e-10</td>\n",
       "      <td>8.380000e-12</td>\n",
       "      <td>1.090000e-07</td>\n",
       "      <td>1.340000e-07</td>\n",
       "      <td>8.790000e-09</td>\n",
       "      <td>6.220000e-09</td>\n",
       "      <td>4.050000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.700000e-10</td>\n",
       "      <td>9.990000e-01</td>\n",
       "      <td>1.470000e-05</td>\n",
       "      <td>2.090000e-04</td>\n",
       "      <td>1.760000e-07</td>\n",
       "      <td>5.300000e-06</td>\n",
       "      <td>3.010000e-06</td>\n",
       "      <td>1.580000e-05</td>\n",
       "      <td>1.080000e-03</td>\n",
       "      <td>4.700000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.900000e-06</td>\n",
       "      <td>5.790000e-09</td>\n",
       "      <td>1.030000e-04</td>\n",
       "      <td>6.990000e-07</td>\n",
       "      <td>9.990000e-01</td>\n",
       "      <td>1.450000e-05</td>\n",
       "      <td>1.750000e-04</td>\n",
       "      <td>2.730000e-05</td>\n",
       "      <td>3.750000e-05</td>\n",
       "      <td>2.500000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.460000e-16</td>\n",
       "      <td>3.990000e-07</td>\n",
       "      <td>1.630000e-11</td>\n",
       "      <td>5.370000e-15</td>\n",
       "      <td>1.790000e-08</td>\n",
       "      <td>1.830000e-09</td>\n",
       "      <td>1.580000e-10</td>\n",
       "      <td>5.540000e-09</td>\n",
       "      <td>1.280000e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2             3             4  \\\n",
       "0  3.860000e-07  9.990000e-01  8.420000e-06  1.340000e-05  2.100000e-05   \n",
       "1  1.000000e+00  9.900000e-16  2.830000e-07  2.560000e-10  8.380000e-12   \n",
       "2  2.700000e-10  9.990000e-01  1.470000e-05  2.090000e-04  1.760000e-07   \n",
       "3  3.900000e-06  5.790000e-09  1.030000e-04  6.990000e-07  9.990000e-01   \n",
       "4  1.000000e+00  3.460000e-16  3.990000e-07  1.630000e-11  5.370000e-15   \n",
       "\n",
       "              5             6             7             8             9  \n",
       "0  3.390000e-05  7.740000e-05  7.080000e-06  1.140000e-03  2.060000e-07  \n",
       "1  1.090000e-07  1.340000e-07  8.790000e-09  6.220000e-09  4.050000e-05  \n",
       "2  5.300000e-06  3.010000e-06  1.580000e-05  1.080000e-03  4.700000e-07  \n",
       "3  1.450000e-05  1.750000e-04  2.730000e-05  3.750000e-05  2.500000e-04  \n",
       "4  1.790000e-08  1.830000e-09  1.580000e-10  5.540000e-09  1.280000e-07  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.zeros(42000)\n",
    "pred_df= pd.read_csv('y_predicted.csv')\n",
    "pred = pred_df.to_numpy() \n",
    "predictions = np.argmax(pred, axis=1)\n",
    "train_pred_df = pd.DataFrame(predictions)\n",
    "train_pred_df_squeezed = train_pred_df.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55994ff0-d5cb-4214-9291-0996bd10122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correct Predictions: 41277\n",
      "Accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = (y_train == train_pred_df_squeezed)\n",
    "num_correct = np.sum(correct_predictions)\n",
    "accuracy = num_correct / y_train.shape[0]\n",
    "print(f\"Number of Correct Predictions: {num_correct}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f2162bf8-4863-40a6-b408-e481b9cbace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =sigmoid(test_df)\n",
    "y_test_pred_logits = model.predict(test_df)\n",
    "test_predictions = np.argmax(y_test_pred_logits, axis=1)\n",
    "test_pred_df = pd.DataFrame(test_predictions)\n",
    "np.savetxt(\"test_predictions.csv\", test_pred_df, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f599ec4-204a-4a70-ab6f-ebc661aa082d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
